---
title: "In Class Exercise 07"
author: "Henry Low"
date: "Oct 14 2024"
date-modified: "last-modified"
execute:
  evalu: true
  echo: true
  message: false
  freeze: true
format: html
editor: visual
---

# Data Sources

*(saved under 'data' folder)*\
From [data.gov.sg](https://data.gov.sg/):\
- [Master Plan 2014 Subzone Boundary (Web) (SHP)](https://data.gov.sg/datasets?query=Master+Plan+2014+Subzone+Boundary&page=1&resultId=d_d14da225fccf921049ab64238ff473d9)

Others:\
condo_resale_2015 in csv format (i.e. condo_resale_2015.csv)

# Setting Up

## Load Packages

-   `sf` package to perform geospatial wrangling tasks 
-   `corrplot` package for multivariate data visualisation and analysis 
-   `tidyverse` package for reading csv files, dataframe processing tasks 
-   `ggpubr` package to assist in plotting 
-   `performance` package to ?
-   `ggpubr` package to assist in plotting 
-   `olsrr` package for building OLS and performing diagnostics tests 
-   `GWModel` package for calibrating geographical weighted family of models 
-   `tmap` package for plotting tasks 

```{r}
pacman::p_load(olsrr, GWmodel, 
               performance,
               sfdep, 
               tidyverse, sf,
               ggstatsplot, corrplot, tmap, gtsummary, ggpubr, see)
```

## Import geospatial data

Lets import the geospatial data, remembering to do the projection to 3414.

```{r}
# Import geospatial data
mpsz = st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL") %>%
  st_transform(3414)
```

## Importing Aspatial data

Lets import the aspatial data and check it.

```{r}
# Import aspatial data
condo_resale = read_csv("data/aspatial/Condo_resale_2015.csv")

# Check imported data
glimpse(condo_resale)
```

Since the file is in csv, there is no metadata to indicate what the CRS is. Therefore, I need to assign a CRS first (4326) before doing the projection. The "4326" is best guess estimate and in practice I would have to plot to see if the coordinates fall within the expected plot to verify.

```{r}
# Convert tibble to sf 
condo_resale.sf <- st_as_sf(condo_resale,
                            coords = c("LONGITUDE", "LATITUDE"),
                            crs=4326) %>%
  st_transform(crs=3414)

# Check result
head(condo_resale.sf)
```

## Save processed datasets

Now once basic data processing is completed, I will proceed to save it in rds format for easy retrieval.

```{r}
write_rds(condo_resale, "data/processed/condo_resale.rds")
write_rds(condo_resale.sf, "data/processed/condo_resale.sf.rds")
write_rds(mpsz, "data/processed/mpsz.rds")
```

# Exploratory Data Analysis (EDA)

First, I'll clear the R cache and load in the datasets

```{r}
# Clear R console
rm(list = ls(all.names = TRUE))

# Load datasets
condo_resale <- read_rds("data/processed/condo_resale.rds")
condo_resale.sf <- read_rds("data/processed/condo_resale.sf.rds")
mpsz <- read_rds("data/processed/mpsz.rds")
```

`ggcorrmat()` from `ggstatsplot` package is used to build the correlation matrix

```{r, fig.height=10, fig.width=12}
# Build correlation matrix
ggcorrmat(condo_resale[, 5:23])
```

::: note
Dont need to worry about VIF for K-means, but it is important in multiple linear regression.
:::

From this, we can see that there is very high correlation between LEASEHOLD_99YR and FREEHOLD. But does this matter? We will find later.

# Building Multiple Linear Regression Model

`lm()` is used to create the multiple linear regression model. After that, `summary()` is used to check the results

```{r}
# Build multiple lienar regression model
condo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + 
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + 
                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + 
                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + 
                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD + LEASEHOLD_99YR,  # Add in LEASEHOLD_99YR to test VIF later
                data=condo_resale.sf)

# Check results
summary(condo.mlr)
```

::: note
Output is an LM object with various important information - residual: to be used later in geospatial - fitted value: estimated price
:::

# Model Assessment (Olsrr)

In this section, we will make use of `olsrr()`. It has a variety of useful methods to build better models.

## Tidy Report

First, `ols_regress()` will be used to print out a nice report.

```{r}
# Print report
ols_regress(condo.mlr)
```

From the report, it is clear that not all variables are statistically significant.

## Multicolinearity Test

`ols_vif_tol()` is then used to check for multicolinearity.

```{r}
# Multicolinearity Test
ols_vif_tol(condo.mlr)
```

Even though FREEHOLD and LEASEHOLD_99YR is highly correlated, since VIF shows that they are \<5, we dont have to remove either one.

## Variable Selection

For variable selection, olsrr provides a few options

1.  Forward stepwise - Include 1 by 1.
2.  Backward stepwise - Remove 1 by 1/
3.  Mix stepwise - take out with replacement

Normally we use forward_p since we want to focus on the variable being statistically significant.

```{r}
# Forward stepwise regression
condo_fw_mlr <- ols_step_forward_p(
  condo.mlr,
  p_val = 0.05,
  details = FALSE # True if want to see the output each step
)
```

Checking the results, I can just print the output.

```{r, fig.height = 10, fig.width=12}
condo_fw_mlr
```

The `plot()` function can be used to see how the statistical results change with each step.

```{r, fig.height=10, fig.width=12}
plot(condo_fw_mlr)
```

Model parameters can also be visualised with `ggcoefstats()`

```{r, fig.height = 10, fig.width=12}
# Plot coefficient statistics
ggcoefstats(condo_fw_mlr$model, sort = "ascending")
```

## Non-Linearity Test

In the context of multiple linear regression, it is important to test model created to ensure that the assumption that linearity and additivity of the relationship between dependent and independent variables hold.

`ols_plot_resid_fit()` from `olsrr` package is used to visualise the relationship between residual and fitted values

```{r}
# Plot residual vs fitted values
ols_plot_resid_fit(condo_fw_mlr$model)
```

Since majority of the point hover around the red line, it means that the model conforms to the linearity assumption.

## Normality Assumption Test

`ols_plot_resid_hist()` is then used to test the normality assumption.

```{r}
# Normality assumption test visualization
ols_plot_resid_hist(condo_fw_mlr$model)
```

This shows that the residual resembles a normal distribution. Another way of doing this test is using `ols_test-normality()`.

```{r}
# Alternative normality assumption test
ols_test_normality(condo_fw_mlr$model)
```

Since the summary table shows the p-values of all tests being smaller than alpha value of 0.05, I can reject the null hypothesis and infer that there is statistical evidence that residuals are not normally distributed.

## Spatial Autocorrelation Test

Since the context of hedonic modelling is for geospatial analysis, we need to visualize the residual of the model

First, I will export the residual and save it as a dataframe.

```{r}
# Export residual
mlr_output <- as.data.frame(condo_fw_mlr$model$residuals) %>%
  rename(`FW_MLR_RES` = `condo_fw_mlr$model$residuals`)
```

Then I'll join the dataframe with condo_resale_sf object.

```{r}
# Join residual dataframe with sf dataframe
condo_resale_sf <- cbind(condo_resale.sf, mlr_output$FW_MLR_RES) %>%
  rename(`MLR_RES` = `mlr_output.FW_MLR_RES`)
```

Finally, i will use tmap to display the distribution of the residuals on an interactive map.

```{r}
# Set to interactive
tmap_mode("view")

# Visualzize point symbol map
tm_shape(mpsz)+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.4) +
tm_shape(condo_resale_sf) + 
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style = "quantile") 

# Check tmap mode back to plot
tmap_mode("plot")
```

The map looks like there are some spatial autocorrelation. More investigation needs to be done.

## Spatial Stationary Test

To prove that there are indeed spatial autocorrelation, Moran's I test will be used. Spatial weights are created with `sfdep` package with `st_weights()` on the neighbour list created with `st_knn()`. Longlat is set to be FALSE because we dont want it to do the projection.

```{r}
# Create spatial weights
condo_resale_sf <- condo_resale_sf %>%
  mutate(nb = st_knn(geometry, k = 6, longlat = FALSE),
         wt = st_weights(nb, style = "W"),
         .before = 1)
```

`global_moran_perm()` from `sfdep` package will then be used to perform the global Moran's I test.

```{r}
# Set seed
set.seed(42)

# Perform Global Moran's I test
global_moran <- global_moran_perm(condo_resale_sf$MLR_RES,
                  condo_resale_sf$nb,
                  condo_resale_sf$wt,
                  alternative = "two.sided",
                  nsim = 99)
global_moran
```

Since the observed Global Moran's I test shows that the p-value is less than the alpha value of 0.05, I will reject the null hypothesis that the residuals are randomly distributed. With a observed global Moran I test statistic of 0.32254, which is greater than 0, I can infer that the residuals resemble cluster distribution.

# Building Hedonic Pricing Models (GWmodel)

In this section, I will model hedonic pricing with a geographically weighted regression model.

## Build Fixed Bandwith GWR Model

`bw.gwr()` from `GWmodel` package is used to determine the optiaml fixed bandwidth for the model. While there are 2 ways to determine the stopping rule:

-   CV - cross-validatiation approach
-   AIC - AIC corrected (AICc) approach

I will use CV method.

```{r}
# Calculate optimal fixed bandwidth
bw_fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                     PROX_CBD + PROX_CHILDCARE + 
                     PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + 
                     PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + 
                     PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                     NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,
                   data = condo_resale_sf,
                   approach = "CV",
                   kernel="gaussian",
                   adaptive = FALSE,
                   longlat=FALSE)
```

This shows that the recommended bandwidth is 971.3405 meters. ::: note See st_crs - CS length is in meters :::

Now the fixed bandwidth can be used to calibrate the model with `gwr.basic()`. The output is saved as a class `gwrm`. We can check it by printing. It contains a SDF dataframe which has a lot of useful information.

```{r}
# Calibrate model with fixed bandwidth
gwr_fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                     PROX_CBD + PROX_CHILDCARE + 
                     PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + 
                     PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + 
                     PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                     NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,
                   data = condo_resale_sf,
                   bw = bw_fixed,
                   kernel="gaussian",
                   adaptive = FALSE,
                   longlat=FALSE)

# Check results
gwr_fixed
```

R-square improved form 0.64 to 0.84, which is very significant.

## Build Adaptive Bandwidth GWR Model

Now, I will repeat the same but for adaptive bandwidth this time.

```{r}
# Select optimal adaptive bandwidth
bw_adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                     PROX_CBD + PROX_CHILDCARE + 
                     PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + 
                     PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + 
                     PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                     NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,
                   data = condo_resale_sf,
                   approach = "CV",
                   kernel="gaussian",
                   adaptive = TRUE,
                   longlat=FALSE)
```

```{r}
gwr_adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                     PROX_CBD + PROX_CHILDCARE + 
                     PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + 
                     PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + 
                     PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                     NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,
                   data = condo_resale_sf,
                   bw = bw_adaptive,
                   kernel="gaussian",
                   adaptive = TRUE,
                   longlat=FALSE)

# Check results
gwr_adaptive
```

## Visualizing GWR Output

As mentioned earlier, the output feature class table contains many useful information

-   Condition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.
-   Local R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.
-   Predicted: these are the estimated (or fitted) y values 3. computed by GWR.
-   Residuals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.
-   Coefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.

To visualize the fields, I will first need to convert it into a sf dataframe.

```{r}
# Select
gwr_adaptive_output <- as.data.frame(gwr_adaptive$SDF) %>%
  select(-c(2:15)) # To remove redundant variables
```

Then i will column bind it to the sf dataframe. `glimpse()` is then used to check the contents.

```{r}
# Combine dataframes
gwr_sf_adaptive <- cbind(condo_resale_sf,
                      gwr_adaptive_output)

# Check result
glimpse(gwr_sf_adaptive)
```

`summary()` is also useful to check the predicted values.

```{r}
# Check yhat
summary(gwr_sf_adaptive$yhat)
```

To visualise the local R2, i will use the `tmap` package.

```{r}
# Plot R2
tm_shape(mpsz)+
  tm_polygons()+
tm_shape(gwr_sf_adaptive) + 
  tm_bubbles(col = "Local_R2",
           size = 0.15,
           border.col = "gray60",
           border.lwd = 1)
```

Now i will try to plot them side by side.

```{r}
# Visualize side by side
tmap_options(check.and.fix = TRUE)
# Set to interactive plot
tmap_mode("view")

# Plot AREA_SQM_SE coefficients
AREA_SQM_SE <- tm_shape(mpsz)+
  tm_polygons(alpha = 0.1) +
tm_shape(gwr_sf_adaptive) +  
  tm_dots(col = "AREA_SQM_SE",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

# Plot AREA_SQM_TV coefficients
AREA_SQM_TV <- tm_shape(mpsz)+
  tm_polygons(alpha = 0.1) +
tm_shape(gwr_sf_adaptive) +  
  tm_dots(col = "AREA_SQM_TV",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

# Show them side by side
tmap_arrange(AREA_SQM_SE, AREA_SQM_TV, 
             asp=1, ncol=2,
             sync = TRUE)
```

This can also be done by region

```{r}
# View central region
tm_shape(mpsz[mpsz$REGION_N=="CENTRAL REGION", ])+
  tm_polygons()+
tm_shape(gwr_sf_adaptive) + 
  tm_bubbles(col = "Local_R2",
           size = 0.15,
           border.col = "gray60",
           border.lwd = 1)
```
